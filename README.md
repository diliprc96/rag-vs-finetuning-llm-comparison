# rag-vs-finetuning-llm-comparison

This study compares four knowledge-injection strategies—Base LLM, Fine-tuned LLM, Base+RAG, and Fine-tuned+RAG—on domain-specific physics question answering, using OpenStax as a controlled corpus.

Initially to prepare the dataset I am using a seperate branch and will merge once the logic is ready. I am trying to use web crawling methods to prepare the dataset, if it is not possible then I will use pdf extraction to create my own custome dataset.